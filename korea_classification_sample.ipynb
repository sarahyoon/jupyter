{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a9d602-aafc-4b27-8e08-6d19a4de8549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datetime\n",
      "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting zope.interface (from datetime)\n",
      "  Downloading zope.interface-7.1.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: pytz in /Users/sarahyoon/Library/Python/3.9/lib/python/site-packages (from datetime) (2024.2)\n",
      "Requirement already satisfied: setuptools in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from zope.interface->datetime) (58.0.4)\n",
      "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
      "Downloading zope.interface-7.1.1-cp39-cp39-macosx_10_9_x86_64.whl (208 kB)\n",
      "Installing collected packages: zope.interface, datetime\n",
      "Successfully installed datetime-5.5 zope.interface-7.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ff9651-8c30-4c7d-a1a4-58f333728b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38ffab8e-99eb-41e9-b28a-916b054611cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "#file_path = '/Users/kyunghwanoh/Project/test/korean_emotional.xlsx'\n",
    "data = pd.read_csv('total_train.csv')\n",
    "data = data[['Sentence', 'Emotion']].dropna()\n",
    "\n",
    "# 라벨 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "data['Emotion'] = label_encoder.fit_transform(data['Emotion'])\n",
    "\n",
    "# Train-test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    data['Sentence'], data['Emotion'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68d5f035-2a3e-42fc-a904-302e41c36933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. Hugging Face Tokenizer 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# 3. 데이터셋 변환 함수\n",
    "def convert_to_hf_dataset(texts, labels, tokenizer, max_length=128):\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    hf_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    })\n",
    "    return hf_dataset\n",
    "\n",
    "    # Hugging Face Dataset으로 변환\n",
    "train_dataset = convert_to_hf_dataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n",
    "val_dataset = convert_to_hf_dataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "176a33e9-c0cc-4333-a306-37e6b50c100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. 모델 로드\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-multilingual-cased',\n",
    "    num_labels=len(label_encoder.classes_)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e2dc55d-3ff8-4c1d-9559-4e9093c0621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모댈 학습 설정\n",
    "\n",
    "\n",
    "# 오늘 날짜를 YYYYMMDD 형식으로 설정\n",
    "today_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "batch_size = 32\n",
    "# TrainingArguments 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'{today_date}',  # output_dir 설정\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,  # 최대 저장 수를 2개로 제한\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"  # wandb 비활성화\n",
    ")\n",
    "\n",
    "# 6. Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e9eae8-c958-4de1-82cc-dd5affd8ea53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='9730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  26/9730 10:57 < 73:53:56, 0.04 it/s, Epoch 0.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "107c3ea4-0716-4377-9c95-4a47ed50ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ㅊㅊ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 감정: ['슬픔']\n"
     ]
    }
   ],
   "source": [
    "#예측 및 활용\n",
    "def predict(text, model, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128\n",
    "    ).to(\"mps\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return label_encoder.inverse_transform([probs.argmax().item()])\n",
    "\n",
    "# 예시\n",
    "new_text = input()\n",
    "predicted_emotion = predict(new_text, model, tokenizer)\n",
    "print(f\"예측 감정: {predicted_emotion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181bd5a-2f68-4b1f-ba78-6b4ffa5b7679",
   "metadata": {},
   "source": [
    "---\n",
    "# 학습된 마지막 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d3182b-0c9f-4b98-bf53-5e2eabb323f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "print(torch.backends.mps.is_available())  # True여야 함\n",
    "print(torch.backends.mps.is_built())     # True여야 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba80dfa-7eed-4540-aec3-639a022a7027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 최근 체크포인트: /Users/kyunghwanoh/Project/notebook/weights/class_241117/checkpoint-965\n",
      "사용 장치: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 로드 및 전처리\n",
    "file_path = '/Users/kyunghwanoh/Project/test/korean_emotional.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "data = data[['Sentence', 'Emotion']].dropna()\n",
    "\n",
    "# 라벨 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "data['Emotion'] = label_encoder.fit_transform(data['Emotion'])\n",
    "\n",
    "# Train-test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    data['Sentence'], data['Emotion'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. 체크포인트 로드 함수\n",
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoints = [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint\")]\n",
    "    if not checkpoints:\n",
    "        raise ValueError(\"체크포인트가 없습니다.\")\n",
    "    latest_checkpoint = max(checkpoints, key=os.path.getmtime)\n",
    "    return latest_checkpoint\n",
    "\n",
    "# 3. 모델과 토크나이저 로드\n",
    "model_checkpoint_dir = '/Users/kyunghwanoh/Project/notebook/weights/class_241117'\n",
    "latest_checkpoint = get_latest_checkpoint(model_checkpoint_dir)\n",
    "print(f\"가장 최근 체크포인트: {latest_checkpoint}\")\n",
    "\n",
    "# 장치 설정 (MPS, CUDA, CPU 중 사용 가능한 장치 선택)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 장치: {device}\")\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "load_model = BertForSequenceClassification.from_pretrained(latest_checkpoint).to(device)\n",
    "load_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49badd7f-0e4e-4bd5-911f-2b92e4135b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텍스트:   진짜 짜증\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 감정: ['분노']\n"
     ]
    }
   ],
   "source": [
    "# 4. 예측 함수\n",
    "def predict(text, load_model, load_tokenizer):\n",
    "    load_model.eval()\n",
    "    inputs = load_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128\n",
    "    ).to(\"mps\")  # 필요에 따라 'cuda' 또는 'cpu'로 변경\n",
    "    outputs = load_model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return label_encoder.inverse_transform([probs.argmax().item()])\n",
    "\n",
    "# 5. 입력 및 추론\n",
    "new_text = input(\"입력 텍스트: \")\n",
    "predicted_emotion = predict(new_text, load_model, load_tokenizer)\n",
    "print(f\"예측 감정: {predicted_emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf710bf9-5b7e-4f02-ba65-ddb610b113ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
