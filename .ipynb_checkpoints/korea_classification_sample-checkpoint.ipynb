{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ff9651-8c30-4c7d-a1a4-58f333728b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ffab8e-99eb-41e9-b28a-916b054611cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "file_path = '/Users/kyunghwanoh/Project/test/korean_emotional.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "data = data[['Sentence', 'Emotion']].dropna()\n",
    "\n",
    "# 라벨 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "data['Emotion'] = label_encoder.fit_transform(data['Emotion'])\n",
    "\n",
    "# Train-test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    data['Sentence'], data['Emotion'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5f035-2a3e-42fc-a904-302e41c36933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Hugging Face Tokenizer 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# 3. 데이터셋 변환 함수\n",
    "def convert_to_hf_dataset(texts, labels, tokenizer, max_length=128):\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    hf_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    })\n",
    "    return hf_dataset\n",
    "\n",
    "    # Hugging Face Dataset으로 변환\n",
    "train_dataset = convert_to_hf_dataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n",
    "val_dataset = convert_to_hf_dataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a33e9-c0cc-4333-a306-37e6b50c100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. 모델 로드\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-multilingual-cased',\n",
    "    num_labels=len(label_encoder.classes_)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2dc55d-3ff8-4c1d-9559-4e9093c0621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모댈 학습 설정\n",
    "\n",
    "\n",
    "# 오늘 날짜를 YYYYMMDD 형식으로 설정\n",
    "today_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "batch_size = 32\n",
    "# TrainingArguments 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'/Users/kyunghwanoh/Project/notebook/weights/class_{today_date}',  # output_dir 설정\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,  # 최대 저장 수를 2개로 제한\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"  # wandb 비활성화\n",
    ")\n",
    "\n",
    "# 6. Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e9eae8-c958-4de1-82cc-dd5affd8ea53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='618' max='4825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 618/4825 2:53:59 < 19:48:16, 0.06 it/s, Epoch 0.64/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1939\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1940\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1941\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1942\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1943\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2320\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   2321\u001b[0m         amp\u001b[38;5;241m.\u001b[39mmaster_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer),\n\u001b[1;32m   2322\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   2323\u001b[0m     )\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   2326\u001b[0m         model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m   2327\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   2328\u001b[0m     )\n\u001b[1;32m   2330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2331\u001b[0m     is_accelerate_available()\n\u001b[1;32m   2332\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2333\u001b[0m ):\n\u001b[1;32m   2334\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_global_grad_norm()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/accelerate/accelerator.py:2347\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2345\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mclip_grad_norm_(max_norm, norm_type)\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_gradients()\n\u001b[0;32m-> 2347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:30\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:92\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m         )\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend([torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(g, norm_type) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_grads])\n\u001b[1;32m     94\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\n\u001b[1;32m     95\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack([norm\u001b[38;5;241m.\u001b[39mto(first_device) \u001b[38;5;28;01mfor\u001b[39;00m norm \u001b[38;5;129;01min\u001b[39;00m norms]), norm_type\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogical_or(total_norm\u001b[38;5;241m.\u001b[39misnan(), total_norm\u001b[38;5;241m.\u001b[39misinf()):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:92\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m         )\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend([torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(g, norm_type) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_grads])\n\u001b[1;32m     94\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\n\u001b[1;32m     95\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack([norm\u001b[38;5;241m.\u001b[39mto(first_device) \u001b[38;5;28;01mfor\u001b[39;00m norm \u001b[38;5;129;01min\u001b[39;00m norms]), norm_type\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogical_or(total_norm\u001b[38;5;241m.\u001b[39misnan(), total_norm\u001b[38;5;241m.\u001b[39misinf()):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "107c3ea4-0716-4377-9c95-4a47ed50ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ㅊㅊ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 감정: ['슬픔']\n"
     ]
    }
   ],
   "source": [
    "#예측 및 활용\n",
    "def predict(text, model, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128\n",
    "    ).to(\"mps\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return label_encoder.inverse_transform([probs.argmax().item()])\n",
    "\n",
    "# 예시\n",
    "new_text = input()\n",
    "predicted_emotion = predict(new_text, model, tokenizer)\n",
    "print(f\"예측 감정: {predicted_emotion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181bd5a-2f68-4b1f-ba78-6b4ffa5b7679",
   "metadata": {},
   "source": [
    "---\n",
    "# 학습된 마지막 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d3182b-0c9f-4b98-bf53-5e2eabb323f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "print(torch.backends.mps.is_available())  # True여야 함\n",
    "print(torch.backends.mps.is_built())     # True여야 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba80dfa-7eed-4540-aec3-639a022a7027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 최근 체크포인트: /Users/kyunghwanoh/Project/notebook/weights/class_241117/checkpoint-965\n",
      "사용 장치: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 로드 및 전처리\n",
    "file_path = '/Users/kyunghwanoh/Project/test/korean_emotional.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "data = data[['Sentence', 'Emotion']].dropna()\n",
    "\n",
    "# 라벨 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "data['Emotion'] = label_encoder.fit_transform(data['Emotion'])\n",
    "\n",
    "# Train-test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    data['Sentence'], data['Emotion'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. 체크포인트 로드 함수\n",
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoints = [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint\")]\n",
    "    if not checkpoints:\n",
    "        raise ValueError(\"체크포인트가 없습니다.\")\n",
    "    latest_checkpoint = max(checkpoints, key=os.path.getmtime)\n",
    "    return latest_checkpoint\n",
    "\n",
    "# 3. 모델과 토크나이저 로드\n",
    "model_checkpoint_dir = '/Users/kyunghwanoh/Project/notebook/weights/class_241117'\n",
    "latest_checkpoint = get_latest_checkpoint(model_checkpoint_dir)\n",
    "print(f\"가장 최근 체크포인트: {latest_checkpoint}\")\n",
    "\n",
    "# 장치 설정 (MPS, CUDA, CPU 중 사용 가능한 장치 선택)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 장치: {device}\")\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "load_model = BertForSequenceClassification.from_pretrained(latest_checkpoint).to(device)\n",
    "load_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49badd7f-0e4e-4bd5-911f-2b92e4135b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텍스트:   진짜 짜증\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 감정: ['분노']\n"
     ]
    }
   ],
   "source": [
    "# 4. 예측 함수\n",
    "def predict(text, load_model, load_tokenizer):\n",
    "    load_model.eval()\n",
    "    inputs = load_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128\n",
    "    ).to(\"mps\")  # 필요에 따라 'cuda' 또는 'cpu'로 변경\n",
    "    outputs = load_model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return label_encoder.inverse_transform([probs.argmax().item()])\n",
    "\n",
    "# 5. 입력 및 추론\n",
    "new_text = input(\"입력 텍스트: \")\n",
    "predicted_emotion = predict(new_text, load_model, load_tokenizer)\n",
    "print(f\"예측 감정: {predicted_emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf710bf9-5b7e-4f02-ba65-ddb610b113ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
